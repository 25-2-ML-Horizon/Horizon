{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOCZ9I10JySKf2M6uUuBecK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 1. combine `mon_features.pkl` & `unmon_features.pkl` into `features_df`"],"metadata":{"id":"m5RXDI0jQISa"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import pickle\n","\n","MON_FILE_PATH = '/content/mon_features.pkl'\n","UNMON_FILE_PATH = '/content/unmon_features.pkl'\n","\n","LABEL_COLUMN = ['website_label', 'monitored_label']\n","\n","FEATURES_VER1 = [\n","    'total_transmission_time', 'std_inter_packet_time',\n","    'avg_outgoing_burst_size', 'avg_incoming_burst_size',\n","    'num_outgoing_packets', 'cumul_max',\n","    'outgoing_first_30', 'avg_incoming_order_first_30'\n","]\n","\n","mon_features_df = pd.read_pickle(MON_FILE_PATH)\n","unmon_features_df = pd.read_pickle(UNMON_FILE_PATH)\n","\n","features_df = pd.concat([mon_features_df, unmon_features_df], ignore_index=True)\n","\n","X = features_df[FEATURES_VER1]\n","y = features_df[LABEL_COLUMN[0]]\n","\n","print(X)\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"405NfQbMQLcZ","executionInfo":{"status":"ok","timestamp":1764066982880,"user_tz":-540,"elapsed":2824,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"f3b600e7-2d29-4072-a4f5-1fabc1c63364"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["       total_transmission_time  std_inter_packet_time  \\\n","0                        10.14               0.041168   \n","1                        10.16               0.163930   \n","2                        11.11               0.066661   \n","3                        13.36               0.047809   \n","4                        10.64               0.038760   \n","...                        ...                    ...   \n","28995                    32.09               0.163669   \n","28996                    38.62               0.114350   \n","28997                    34.93               1.331199   \n","28998                    11.84               0.083521   \n","28999                     9.62               0.026874   \n","\n","       avg_outgoing_burst_size  avg_incoming_burst_size  num_outgoing_packets  \\\n","0                     1.551282                16.666667                 121.0   \n","1                     1.702128                 9.319149                  80.0   \n","2                     1.552632                16.315789                 118.0   \n","3                     1.525000                16.550000                 122.0   \n","4                     1.455696                16.341772                 115.0   \n","...                        ...                      ...                   ...   \n","28995                 1.619608                16.328125                 413.0   \n","28996                 1.995536                20.724444                 447.0   \n","28997                 2.107143                10.785714                  59.0   \n","28998                 1.714286                 7.375000                  96.0   \n","28999                 1.076923                32.226667                 322.0   \n","\n","       cumul_max  outgoing_first_30  avg_incoming_order_first_30  \n","0            0.0           0.300000                    13.619048  \n","1            0.0           0.266667                    14.318182  \n","2            0.0           0.233333                    14.913043  \n","3            0.0           0.300000                    13.619048  \n","4            0.0           0.266667                    14.318182  \n","...          ...                ...                          ...  \n","28995        0.0           0.333333                    15.750000  \n","28996        0.0           0.366667                    14.631579  \n","28997       -1.0           0.366667                    14.052632  \n","28998       -1.0           0.300000                    14.666667  \n","28999        0.0           0.300000                    15.190476  \n","\n","[29000 rows x 8 columns]\n","0        0\n","1        0\n","2        0\n","3        0\n","4        0\n","        ..\n","28995   -1\n","28996   -1\n","28997   -1\n","28998   -1\n","28999   -1\n","Name: website_label, Length: 29000, dtype: int64\n"]}]},{"cell_type":"markdown","source":["### 2. SVM"],"metadata":{"id":"vbjj7DOy-ajJ"}},{"cell_type":"markdown","source":["Ignore ConvergenceWarning"],"metadata":{"id":"T8DrseX8Xo-X"}},{"cell_type":"code","source":["import warnings\n","from sklearn.exceptions import ConvergenceWarning\n","warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"],"metadata":{"id":"wei-xc2XXotK","executionInfo":{"status":"ok","timestamp":1764066984944,"user_tz":-540,"elapsed":2058,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Split the dataset into training and testing sets"],"metadata":{"id":"ag7TNuQIXttl"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"],"metadata":{"id":"BzTWApNiXtNG","executionInfo":{"status":"ok","timestamp":1764066985888,"user_tz":-540,"elapsed":937,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Map all points to have mean=0 and std=1"],"metadata":{"id":"5tGt6RZyX77d"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scale = scaler.fit_transform(X_train.values)\n","X_test_scale = scaler.transform(X_test.values)\n","X_scale = scaler.fit_transform(X.values)"],"metadata":{"id":"yVgWAxkuX8T7","executionInfo":{"status":"ok","timestamp":1764066985906,"user_tz":-540,"elapsed":13,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Train and test SVM using RBF kernel"],"metadata":{"id":"QQ-XuWD_jrtz"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","clf_rbf = SVC(kernel='rbf', C=100, gamma=0.1, class_weight='balanced')\n","clf_rbf.fit(X_train_scale, y_train)\n","y_pred_rbf = clf_rbf.predict(X_test_scale)\n","\n","print(\"============ Before Hyperparameter tuning ============\")\n","print(\"SVM Accuracy: {}\".format(accuracy_score(y_test, y_pred_rbf)))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rbf))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9q3WURZjgco","executionInfo":{"status":"ok","timestamp":1764067033421,"user_tz":-540,"elapsed":47509,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"f165f936-25bf-4a37-8fbf-6088548d0b04"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["============ Before Hyperparameter tuning ============\n","SVM Accuracy: 0.7558620689655172\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","          -1       0.98      0.73      0.84      2470\n","           0       0.57      0.64      0.61        42\n","           1       0.80      0.80      0.80        51\n","           2       0.31      0.98      0.47        47\n","           3       0.47      0.84      0.61        51\n","           4       0.76      0.79      0.77        43\n","           5       0.75      0.82      0.78        44\n","           6       0.83      1.00      0.91        53\n","           7       0.64      0.65      0.65        55\n","           8       0.64      0.57      0.61        47\n","           9       0.64      0.62      0.63        45\n","          10       0.57      0.73      0.64        44\n","          11       0.58      0.82      0.68        55\n","          12       0.72      0.92      0.81        52\n","          13       0.43      0.42      0.42        57\n","          14       0.61      0.74      0.67        58\n","          15       0.73      0.80      0.76        46\n","          16       0.80      0.68      0.73        59\n","          17       0.51      0.67      0.58        51\n","          18       0.89      0.98      0.93        50\n","          19       0.60      0.62      0.61        45\n","          20       0.85      0.98      0.91        52\n","          21       0.50      0.77      0.61        48\n","          22       0.71      0.79      0.75        47\n","          23       0.63      0.76      0.69        42\n","          24       0.48      0.46      0.47        48\n","          25       0.61      0.50      0.55        54\n","          26       0.76      0.89      0.82        47\n","          27       0.87      0.79      0.83        57\n","          28       0.77      0.85      0.81        48\n","          29       0.67      0.92      0.77        39\n","          30       0.74      0.91      0.81        43\n","          31       0.79      0.78      0.78        49\n","          32       0.72      0.63      0.67        54\n","          33       0.56      0.87      0.68        54\n","          34       0.65      0.41      0.50        49\n","          35       0.46      0.82      0.59        40\n","          36       0.80      0.91      0.85        44\n","          37       0.68      0.53      0.60        60\n","          38       0.62      0.71      0.66        55\n","          39       0.73      0.70      0.71        46\n","          40       0.65      0.69      0.67        58\n","          41       0.82      0.82      0.82        57\n","          42       0.59      0.81      0.68        47\n","          43       0.65      0.90      0.75        49\n","          44       0.82      1.00      0.90        46\n","          45       0.66      0.56      0.60        52\n","          46       0.52      0.76      0.61        45\n","          47       0.73      0.67      0.70        54\n","          48       0.30      0.89      0.45        47\n","          49       0.87      0.65      0.75        52\n","          50       0.86      0.90      0.88        49\n","          51       0.70      0.77      0.73        61\n","          52       0.84      0.96      0.89        53\n","          53       0.54      0.75      0.62        40\n","          54       0.80      0.79      0.80        57\n","          55       0.60      0.63      0.61        54\n","          56       0.87      0.98      0.92        49\n","          57       0.90      0.95      0.93        39\n","          58       0.90      0.83      0.86        52\n","          59       0.86      0.86      0.86        51\n","          60       0.86      0.78      0.82        54\n","          61       0.79      0.68      0.73        56\n","          62       0.75      0.79      0.77        61\n","          63       0.77      0.75      0.76        55\n","          64       0.70      0.76      0.73        42\n","          65       0.77      0.61      0.68        59\n","          66       0.66      0.71      0.68        41\n","          67       0.83      0.78      0.80        58\n","          68       0.76      0.72      0.74        47\n","          69       0.43      0.82      0.56        45\n","          70       0.89      1.00      0.94        47\n","          71       0.78      0.89      0.83        45\n","          72       0.78      0.69      0.73        58\n","          73       0.85      0.90      0.87        58\n","          74       0.74      0.74      0.74        66\n","          75       0.65      1.00      0.79        55\n","          76       0.85      0.91      0.88        44\n","          77       0.75      0.51      0.61        59\n","          78       0.62      0.83      0.71        53\n","          79       0.54      0.60      0.57        53\n","          80       0.87      0.89      0.88        53\n","          81       0.74      0.69      0.71        45\n","          82       0.72      0.75      0.73        55\n","          83       0.72      0.78      0.75        50\n","          84       0.79      0.77      0.78        48\n","          85       0.84      0.96      0.90        54\n","          86       0.94      0.96      0.95        51\n","          87       0.48      0.84      0.61        51\n","          88       0.74      0.63      0.68        49\n","          89       0.59      0.52      0.55        44\n","          90       0.75      0.68      0.71        44\n","          91       0.72      0.84      0.78        37\n","          92       0.61      0.65      0.63        52\n","          93       0.88      0.90      0.89        58\n","          94       0.71      0.58      0.64        50\n","\n","    accuracy                           0.76      7250\n","   macro avg       0.71      0.77      0.73      7250\n","weighted avg       0.80      0.76      0.76      7250\n","\n"]}]},{"cell_type":"markdown","source":["### 3. Hyperparameter tuning by using Grid Search"],"metadata":{"id":"e-3oRgY2-og7"}},{"cell_type":"code","source":["from sklearn.experimental import enable_halving_search_cv\n","from sklearn.model_selection import HalvingGridSearchCV\n","\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","              'gamma': [0.001, 0.01, 0.1, 1],\n","              'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n","\n","search = HalvingGridSearchCV(SVC(class_weight='balanced'), param_grid, cv=5, factor=3, n_jobs=-1)\n","search.fit(X_train_scale, y_train)\n","y_pred_tuned = search.predict(X_test_scale)\n","\n","print(\"============ After Hyperparameter tuning ============\")\n","print(\"SVM Accuracy: {}\".format(accuracy_score(y_test, y_pred_tuned)))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tuned))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pE19k57F_PuW","executionInfo":{"status":"ok","timestamp":1764069341834,"user_tz":-540,"elapsed":2308408,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"b81b1d6a-0f82-492c-a23d-4b77980e4276"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["============ After Hyperparameter tuning ============\n","SVM Accuracy: 0.7968275862068965\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","          -1       0.92      0.89      0.90      2470\n","           0       0.74      0.76      0.75        42\n","           1       0.87      0.80      0.84        51\n","           2       0.47      0.91      0.62        47\n","           3       0.63      0.73      0.67        51\n","           4       0.73      0.74      0.74        43\n","           5       0.80      0.89      0.84        44\n","           6       0.89      0.94      0.92        53\n","           7       0.60      0.55      0.57        55\n","           8       0.73      0.74      0.74        47\n","           9       0.59      0.64      0.62        45\n","          10       0.66      0.75      0.70        44\n","          11       0.69      0.75      0.72        55\n","          12       0.82      0.90      0.86        52\n","          13       0.65      0.53      0.58        57\n","          14       0.62      0.71      0.66        58\n","          15       0.72      0.78      0.75        46\n","          16       0.71      0.69      0.70        59\n","          17       0.54      0.61      0.57        51\n","          18       0.92      0.92      0.92        50\n","          19       0.57      0.56      0.56        45\n","          20       0.91      0.96      0.93        52\n","          21       0.61      0.62      0.62        48\n","          22       0.66      0.74      0.70        47\n","          23       0.72      0.79      0.75        42\n","          24       0.45      0.54      0.49        48\n","          25       0.70      0.65      0.67        54\n","          26       0.75      0.81      0.78        47\n","          27       0.90      0.82      0.86        57\n","          28       0.77      0.83      0.80        48\n","          29       0.73      0.92      0.82        39\n","          30       0.74      0.86      0.80        43\n","          31       0.81      0.78      0.79        49\n","          32       0.68      0.63      0.65        54\n","          33       0.65      0.80      0.72        54\n","          34       0.49      0.45      0.47        49\n","          35       0.69      0.82      0.75        40\n","          36       0.87      0.91      0.89        44\n","          37       0.65      0.58      0.61        60\n","          38       0.65      0.56      0.60        55\n","          39       0.77      0.74      0.76        46\n","          40       0.78      0.72      0.75        58\n","          41       0.83      0.84      0.83        57\n","          42       0.65      0.66      0.65        47\n","          43       0.75      0.90      0.81        49\n","          44       0.87      1.00      0.93        46\n","          45       0.45      0.44      0.45        52\n","          46       0.70      0.69      0.70        45\n","          47       0.78      0.67      0.72        54\n","          48       0.51      0.81      0.62        47\n","          49       0.86      0.73      0.79        52\n","          50       0.95      0.82      0.88        49\n","          51       0.68      0.69      0.68        61\n","          52       0.90      0.89      0.90        53\n","          53       0.61      0.68      0.64        40\n","          54       0.82      0.79      0.80        57\n","          55       0.58      0.63      0.60        54\n","          56       0.92      0.98      0.95        49\n","          57       0.78      0.82      0.80        39\n","          58       0.91      0.75      0.82        52\n","          59       0.85      0.90      0.88        51\n","          60       0.82      0.83      0.83        54\n","          61       0.80      0.70      0.74        56\n","          62       0.73      0.66      0.69        61\n","          63       0.89      0.76      0.82        55\n","          64       0.61      0.79      0.69        42\n","          65       0.81      0.59      0.69        59\n","          66       0.81      0.71      0.75        41\n","          67       0.78      0.79      0.79        58\n","          68       0.63      0.55      0.59        47\n","          69       0.70      0.78      0.74        45\n","          70       0.94      0.96      0.95        47\n","          71       0.67      0.76      0.71        45\n","          72       0.79      0.76      0.77        58\n","          73       0.85      0.81      0.83        58\n","          74       0.68      0.59      0.63        66\n","          75       0.93      1.00      0.96        55\n","          76       0.91      0.93      0.92        44\n","          77       0.65      0.58      0.61        59\n","          78       0.69      0.77      0.73        53\n","          79       0.56      0.58      0.57        53\n","          80       0.96      0.91      0.93        53\n","          81       0.70      0.62      0.66        45\n","          82       0.74      0.67      0.70        55\n","          83       0.82      0.74      0.78        50\n","          84       0.88      0.75      0.81        48\n","          85       0.81      0.96      0.88        54\n","          86       0.94      0.88      0.91        51\n","          87       0.72      0.84      0.77        51\n","          88       0.74      0.63      0.68        49\n","          89       0.60      0.48      0.53        44\n","          90       0.77      0.68      0.72        44\n","          91       0.73      0.89      0.80        37\n","          92       0.80      0.71      0.76        52\n","          93       0.96      0.88      0.92        58\n","          94       0.77      0.66      0.71        50\n","\n","    accuracy                           0.80      7250\n","   macro avg       0.74      0.75      0.74      7250\n","weighted avg       0.80      0.80      0.80      7250\n","\n"]}]}]}