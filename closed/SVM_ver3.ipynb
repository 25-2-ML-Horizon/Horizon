{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["u1ofSwdQ9Ner"],"authorship_tag":"ABX9TyMAhWtf8wiNPa2h5rhgJ1Br"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 1. mon_feature.pkl upload"],"metadata":{"id":"m5RXDI0jQISa"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import pickle\n","\n","FILE_PATH = '/content/mon_features.pkl'\n","LABEL_COLUMN = ['website_label', 'monitored_label']\n","\n","FEATURES_VER3 = [\n","    'total_transmission_time', 'std_inter_packet_time', 'avg_outgoing_burst_size',\n","    'avg_incoming_burst_size', 'num_outgoing_packets', 'incoming_packet_ratio',\n","    'outgoing_packet_ratio', 'cumul_packets_10pct', 'cumul_packets_30pct',\n","    'outgoing_order_skew', 'incoming_order_skew', 'cumul_max',\n","    'num_incoming_first_30', 'outgoing_first_30', 'avg_incoming_order_first_30',\n","    'avg_outgoing_order_first_30'\n","]\n","\n","features_df = pd.read_pickle(FILE_PATH)\n","X = features_df[FEATURES_VER3]\n","y = features_df[LABEL_COLUMN[0]]\n","\n","print(X)\n","print(y)"],"metadata":{"id":"405NfQbMQLcZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763876670584,"user_tz":-540,"elapsed":2173,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"de3e8bce-3856-4771-e2bb-367818255fdf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["       total_transmission_time  std_inter_packet_time  \\\n","0                        10.14               0.041168   \n","1                        10.16               0.163930   \n","2                        11.11               0.066661   \n","3                        13.36               0.047809   \n","4                        10.64               0.038760   \n","...                        ...                    ...   \n","18995                    43.91               0.143962   \n","18996                    15.60               0.019465   \n","18997                    14.93               0.016411   \n","18998                    19.91               0.033281   \n","18999                    13.76               0.011074   \n","\n","       avg_outgoing_burst_size  avg_incoming_burst_size  num_outgoing_packets  \\\n","0                     1.551282                16.666667                 121.0   \n","1                     1.702128                 9.319149                  80.0   \n","2                     1.552632                16.315789                 118.0   \n","3                     1.525000                16.550000                 122.0   \n","4                     1.455696                16.341772                 115.0   \n","...                        ...                      ...                   ...   \n","18995                 1.659517                23.632708                 619.0   \n","18996                 1.529086                25.977901                 552.0   \n","18997                 1.560647                25.264151                 579.0   \n","18998                 1.699507                22.692875                 690.0   \n","18999                 1.869136                22.581281                 757.0   \n","\n","       incoming_packet_ratio  outgoing_packet_ratio  cumul_packets_10pct  \\\n","0                   0.914849               0.085151                 23.0   \n","1                   0.845560               0.154440                  5.0   \n","2                   0.913108               0.086892                  8.0   \n","3                   0.915629               0.084371                 15.0   \n","4                   0.918208               0.081792                 22.0   \n","...                      ...                    ...                  ...   \n","18995               0.934386               0.065614                350.0   \n","18996               0.944556               0.055444                 20.0   \n","18997               0.941821               0.058179                 28.0   \n","18998               0.930486               0.069514                  5.0   \n","18999               0.923728               0.076272                 25.0   \n","\n","       cumul_packets_30pct  outgoing_order_skew  incoming_order_skew  \\\n","0                     55.0            -0.257072             0.021546   \n","1                     49.0             0.153926            -0.037553   \n","2                     34.0            -0.463423             0.038716   \n","3                     57.0            -0.391122             0.031804   \n","4                     53.0            -0.355596             0.027842   \n","...                    ...                  ...                  ...   \n","18995               8497.0             0.049047            -0.007797   \n","18996                204.0             0.242173            -0.013495   \n","18997                282.0             0.160097            -0.010204   \n","18998                162.0            -0.062985             0.001099   \n","18999               1374.0            -0.066961            -0.006460   \n","\n","       cumul_max  num_incoming_first_30  outgoing_first_30  \\\n","0            0.0                   21.0           0.300000   \n","1            0.0                   22.0           0.266667   \n","2            0.0                   23.0           0.233333   \n","3            0.0                   21.0           0.300000   \n","4            0.0                   22.0           0.266667   \n","...          ...                    ...                ...   \n","18995        0.0                   23.0           0.233333   \n","18996        0.0                   23.0           0.233333   \n","18997        0.0                   22.0           0.266667   \n","18998        0.0                   23.0           0.233333   \n","18999        0.0                   23.0           0.233333   \n","\n","       avg_incoming_order_first_30  avg_outgoing_order_first_30  \n","0                        13.619048                    16.555556  \n","1                        14.318182                    15.000000  \n","2                        14.913043                    13.142857  \n","3                        13.619048                    16.555556  \n","4                        14.318182                    15.000000  \n","...                            ...                          ...  \n","18995                    14.913043                    13.142857  \n","18996                    14.913043                    13.142857  \n","18997                    14.409091                    14.750000  \n","18998                    14.913043                    13.142857  \n","18999                    14.913043                    13.142857  \n","\n","[19000 rows x 16 columns]\n","0         0\n","1         0\n","2         0\n","3         0\n","4         0\n","         ..\n","18995    94\n","18996    94\n","18997    94\n","18998    94\n","18999    94\n","Name: website_label, Length: 19000, dtype: int64\n"]}]},{"cell_type":"markdown","source":["### 2. SVM"],"metadata":{"id":"u1ofSwdQ9Ner"}},{"cell_type":"markdown","source":["Ignore ConvergenceWarning"],"metadata":{"id":"T8DrseX8Xo-X"}},{"cell_type":"code","source":["import warnings\n","from sklearn.exceptions import ConvergenceWarning\n","warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"],"metadata":{"id":"wei-xc2XXotK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split the dataset into training and testing sets"],"metadata":{"id":"ag7TNuQIXttl"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"],"metadata":{"id":"BzTWApNiXtNG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Map all points to have mean=0 and std=1"],"metadata":{"id":"5tGt6RZyX77d"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scale = scaler.fit_transform(X_train.values)\n","X_test_scale = scaler.transform(X_test.values)\n","X_scale = scaler.fit_transform(X.values)"],"metadata":{"id":"yVgWAxkuX8T7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train and test SVM using RBF kernel"],"metadata":{"id":"QQ-XuWD_jrtz"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","clf_rbf = SVC(kernel='rbf', C=100, gamma=0.1, class_weight='balanced')\n","clf_rbf.fit(X_train_scale, y_train)\n","y_pred_rbf = clf_rbf.predict(X_test_scale)\n","\n","print(\"============ Before Hyperparameter tuning ============\")\n","print(\"SVM Accuracy: {}\".format(accuracy_score(y_test, y_pred_rbf)))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rbf))"],"metadata":{"id":"k9q3WURZjgco","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763877751705,"user_tz":-540,"elapsed":16395,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"f262d7b7-2edd-44ab-a412-21307d2e0351"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["============ Before Hyperparameter tuning ============\n","SVM Accuracy: 0.816\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.80      0.78      0.79        51\n","           1       0.90      0.77      0.83        47\n","           2       0.85      0.96      0.90        48\n","           3       0.86      1.00      0.93        37\n","           4       0.77      0.82      0.79        44\n","           5       0.89      0.88      0.88        56\n","           6       0.93      0.90      0.91        58\n","           7       0.76      0.77      0.77        53\n","           8       0.67      0.73      0.70        49\n","           9       0.61      0.71      0.66        38\n","          10       0.85      0.85      0.85        55\n","          11       0.74      0.82      0.78        45\n","          12       0.91      0.93      0.92        55\n","          13       0.58      0.62      0.60        45\n","          14       0.66      0.75      0.70        52\n","          15       0.86      0.79      0.83        48\n","          16       0.66      0.78      0.72        50\n","          17       0.79      0.75      0.77        59\n","          18       0.90      0.98      0.94        47\n","          19       0.55      0.61      0.58        46\n","          20       0.96      0.96      0.96        55\n","          21       0.84      0.90      0.87        48\n","          22       0.66      0.76      0.71        46\n","          23       0.87      0.89      0.88        46\n","          24       0.52      0.55      0.54        58\n","          25       0.74      0.67      0.70        48\n","          26       0.80      0.89      0.84        45\n","          27       0.96      0.88      0.92        50\n","          28       0.88      0.88      0.88        42\n","          29       0.87      0.82      0.85        57\n","          30       0.78      0.93      0.85        55\n","          31       0.80      0.93      0.86        43\n","          32       0.74      0.71      0.73        52\n","          33       0.81      0.94      0.87        47\n","          34       0.67      0.62      0.65        53\n","          35       0.91      0.87      0.89        45\n","          36       0.93      0.91      0.92        45\n","          37       0.78      0.73      0.75        62\n","          38       0.69      0.72      0.71        58\n","          39       0.74      0.70      0.72        53\n","          40       0.82      0.77      0.79        60\n","          41       0.85      0.85      0.85        53\n","          42       0.66      0.67      0.67        49\n","          43       0.94      0.96      0.95        46\n","          44       0.98      0.94      0.96        53\n","          45       0.71      0.56      0.62        45\n","          46       0.87      0.77      0.82        44\n","          47       0.76      0.76      0.76        51\n","          48       0.98      0.91      0.94        46\n","          49       0.93      0.81      0.87        52\n","          50       0.87      0.92      0.89        37\n","          51       0.88      0.73      0.80        49\n","          52       0.84      0.91      0.87        57\n","          53       0.85      0.87      0.86        39\n","          54       0.87      0.78      0.82        59\n","          55       0.60      0.57      0.58        44\n","          56       0.92      0.95      0.93        57\n","          57       0.90      0.94      0.92        47\n","          58       0.95      0.85      0.90        48\n","          59       0.89      0.98      0.93        51\n","          60       0.80      0.80      0.80        50\n","          61       0.85      0.75      0.80        68\n","          62       0.81      0.75      0.78        68\n","          63       0.75      0.86      0.80        42\n","          64       0.79      0.86      0.82        49\n","          65       0.82      0.77      0.80        48\n","          66       0.81      0.88      0.85        59\n","          67       0.82      0.90      0.86        50\n","          68       0.87      0.77      0.82        61\n","          69       0.89      0.85      0.87        55\n","          70       0.97      0.87      0.92        45\n","          71       0.70      0.91      0.79        44\n","          72       0.76      0.73      0.75        52\n","          73       0.93      0.93      0.93        54\n","          74       0.81      0.80      0.80        59\n","          75       0.98      1.00      0.99        44\n","          76       0.91      0.94      0.92        51\n","          77       0.70      0.63      0.67        41\n","          78       0.80      0.84      0.82        51\n","          79       0.60      0.50      0.55        42\n","          80       0.96      0.92      0.94        52\n","          81       0.91      0.77      0.83        56\n","          82       0.85      0.70      0.77        57\n","          83       0.69      0.85      0.76        40\n","          84       0.88      0.87      0.88        53\n","          85       0.92      0.96      0.94        56\n","          86       0.84      1.00      0.91        42\n","          87       0.88      0.84      0.86        45\n","          88       0.82      0.84      0.83        44\n","          89       0.64      0.56      0.60        45\n","          90       0.81      0.81      0.81        47\n","          91       0.92      0.84      0.88        56\n","          92       0.70      0.68      0.69        44\n","          93       0.98      0.94      0.96        47\n","          94       0.82      0.73      0.77        55\n","\n","    accuracy                           0.82      4750\n","   macro avg       0.82      0.82      0.81      4750\n","weighted avg       0.82      0.82      0.82      4750\n","\n"]}]},{"cell_type":"markdown","source":["### 3. Hyperparameter tuning by using Grid Search"],"metadata":{"id":"0cDS665_y8tg"}},{"cell_type":"code","source":["from sklearn.experimental import enable_halving_search_cv\n","from sklearn.model_selection import HalvingGridSearchCV\n","\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","              'gamma': [0.001, 0.01, 0.1, 1],\n","              'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n","\n","search = HalvingGridSearchCV(SVC(class_weight='balanced'), param_grid, cv=5, factor=3, n_jobs=-1)\n","search.fit(X_train_scale, y_train)\n","\n","print(search.best_params_)\n","print(search.best_estimator_)"],"metadata":{"id":"uM3lBZ0ohJOh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763878728711,"user_tz":-540,"elapsed":954186,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"04d5f1c3-231d-432d-f43c-bf7d289b0597"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["============ After Hyperparameter tuning ============\n","SVM Accuracy: 0.816\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.80      0.78      0.79        51\n","           1       0.90      0.77      0.83        47\n","           2       0.85      0.96      0.90        48\n","           3       0.86      1.00      0.93        37\n","           4       0.77      0.82      0.79        44\n","           5       0.89      0.88      0.88        56\n","           6       0.93      0.90      0.91        58\n","           7       0.76      0.77      0.77        53\n","           8       0.67      0.73      0.70        49\n","           9       0.61      0.71      0.66        38\n","          10       0.85      0.85      0.85        55\n","          11       0.74      0.82      0.78        45\n","          12       0.91      0.93      0.92        55\n","          13       0.58      0.62      0.60        45\n","          14       0.66      0.75      0.70        52\n","          15       0.86      0.79      0.83        48\n","          16       0.66      0.78      0.72        50\n","          17       0.79      0.75      0.77        59\n","          18       0.90      0.98      0.94        47\n","          19       0.55      0.61      0.58        46\n","          20       0.96      0.96      0.96        55\n","          21       0.84      0.90      0.87        48\n","          22       0.66      0.76      0.71        46\n","          23       0.87      0.89      0.88        46\n","          24       0.52      0.55      0.54        58\n","          25       0.74      0.67      0.70        48\n","          26       0.80      0.89      0.84        45\n","          27       0.96      0.88      0.92        50\n","          28       0.88      0.88      0.88        42\n","          29       0.87      0.82      0.85        57\n","          30       0.78      0.93      0.85        55\n","          31       0.80      0.93      0.86        43\n","          32       0.74      0.71      0.73        52\n","          33       0.81      0.94      0.87        47\n","          34       0.67      0.62      0.65        53\n","          35       0.91      0.87      0.89        45\n","          36       0.93      0.91      0.92        45\n","          37       0.78      0.73      0.75        62\n","          38       0.69      0.72      0.71        58\n","          39       0.74      0.70      0.72        53\n","          40       0.82      0.77      0.79        60\n","          41       0.85      0.85      0.85        53\n","          42       0.66      0.67      0.67        49\n","          43       0.94      0.96      0.95        46\n","          44       0.98      0.94      0.96        53\n","          45       0.71      0.56      0.62        45\n","          46       0.87      0.77      0.82        44\n","          47       0.76      0.76      0.76        51\n","          48       0.98      0.91      0.94        46\n","          49       0.93      0.81      0.87        52\n","          50       0.87      0.92      0.89        37\n","          51       0.88      0.73      0.80        49\n","          52       0.84      0.91      0.87        57\n","          53       0.85      0.87      0.86        39\n","          54       0.87      0.78      0.82        59\n","          55       0.60      0.57      0.58        44\n","          56       0.92      0.95      0.93        57\n","          57       0.90      0.94      0.92        47\n","          58       0.95      0.85      0.90        48\n","          59       0.89      0.98      0.93        51\n","          60       0.80      0.80      0.80        50\n","          61       0.85      0.75      0.80        68\n","          62       0.81      0.75      0.78        68\n","          63       0.75      0.86      0.80        42\n","          64       0.79      0.86      0.82        49\n","          65       0.82      0.77      0.80        48\n","          66       0.81      0.88      0.85        59\n","          67       0.82      0.90      0.86        50\n","          68       0.87      0.77      0.82        61\n","          69       0.89      0.85      0.87        55\n","          70       0.97      0.87      0.92        45\n","          71       0.70      0.91      0.79        44\n","          72       0.76      0.73      0.75        52\n","          73       0.93      0.93      0.93        54\n","          74       0.81      0.80      0.80        59\n","          75       0.98      1.00      0.99        44\n","          76       0.91      0.94      0.92        51\n","          77       0.70      0.63      0.67        41\n","          78       0.80      0.84      0.82        51\n","          79       0.60      0.50      0.55        42\n","          80       0.96      0.92      0.94        52\n","          81       0.91      0.77      0.83        56\n","          82       0.85      0.70      0.77        57\n","          83       0.69      0.85      0.76        40\n","          84       0.88      0.87      0.88        53\n","          85       0.92      0.96      0.94        56\n","          86       0.84      1.00      0.91        42\n","          87       0.88      0.84      0.86        45\n","          88       0.82      0.84      0.83        44\n","          89       0.64      0.56      0.60        45\n","          90       0.81      0.81      0.81        47\n","          91       0.92      0.84      0.88        56\n","          92       0.70      0.68      0.69        44\n","          93       0.98      0.94      0.96        47\n","          94       0.82      0.73      0.77        55\n","\n","    accuracy                           0.82      4750\n","   macro avg       0.82      0.82      0.81      4750\n","weighted avg       0.82      0.82      0.82      4750\n","\n"]}]},{"cell_type":"code","source":["y_pred_tuned = search.predict(X_test_scale)\n","\n","print(\"============ After Hyperparameter tuning ============\")\n","print(\"SVM Accuracy: {}\".format(accuracy_score(y_test, y_pred_tuned)))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tuned))"],"metadata":{"id":"_LYLq-ydnlwS"},"execution_count":null,"outputs":[]}]}