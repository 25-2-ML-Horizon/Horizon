{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQID/T97XmW6/JHNd2AqPx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### 1. mon_feature.pkl upload"],"metadata":{"id":"m5RXDI0jQISa"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import pickle\n","\n","FILE_PATH = '/content/mon_features.pkl'\n","LABEL_COLUMN = ['website_label', 'monitored_label']\n","\n","FEATURES_VER2 = [\n","    'total_transmission_time', 'std_inter_packet_time',\n","    'num_outgoing_packets', 'avg_incoming_burst_size',\n","    'cumul_packets_30pct', 'cumul_packets_10pct',\n","    'incoming_order_skew', 'outgoing_order_skew',\n","    'cumul_max', 'bigram_OO', 'avg_outgoing_order_first_30',\n","    'num_incoming_first_30', 'incoming_packet_ratio'\n","]\n","\n","features_df = pd.read_pickle(FILE_PATH)\n","X = features_df[FEATURES_VER2]\n","y = features_df[LABEL_COLUMN[0]]\n","\n","print(X)\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"405NfQbMQLcZ","executionInfo":{"status":"ok","timestamp":1763946041208,"user_tz":-540,"elapsed":1389,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"e14bc280-ae2a-495c-c725-504939f32297"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["       total_transmission_time  std_inter_packet_time  num_outgoing_packets  \\\n","0                        10.14               0.041168                 121.0   \n","1                        10.16               0.163930                  80.0   \n","2                        11.11               0.066661                 118.0   \n","3                        13.36               0.047809                 122.0   \n","4                        10.64               0.038760                 115.0   \n","...                        ...                    ...                   ...   \n","18995                    43.91               0.143962                 619.0   \n","18996                    15.60               0.019465                 552.0   \n","18997                    14.93               0.016411                 579.0   \n","18998                    19.91               0.033281                 690.0   \n","18999                    13.76               0.011074                 757.0   \n","\n","       avg_incoming_burst_size  cumul_packets_30pct  cumul_packets_10pct  \\\n","0                    16.666667                 55.0                 23.0   \n","1                     9.319149                 49.0                  5.0   \n","2                    16.315789                 34.0                  8.0   \n","3                    16.550000                 57.0                 15.0   \n","4                    16.341772                 53.0                 22.0   \n","...                        ...                  ...                  ...   \n","18995                23.632708               8497.0                350.0   \n","18996                25.977901                204.0                 20.0   \n","18997                25.264151                282.0                 28.0   \n","18998                22.692875                162.0                  5.0   \n","18999                22.581281               1374.0                 25.0   \n","\n","       incoming_order_skew  outgoing_order_skew  cumul_max  bigram_OO  \\\n","0                 0.021546            -0.257072        0.0   0.860563   \n","1                -0.037553             0.153926        0.0   0.756286   \n","2                 0.038716            -0.463423        0.0   0.857775   \n","3                 0.031804            -0.391122        0.0   0.860900   \n","4                 0.027842            -0.355596        0.0   0.862633   \n","...                    ...                  ...        ...        ...   \n","18995            -0.007797             0.049047        0.0   0.894943   \n","18996            -0.013495             0.242173        0.0   0.908287   \n","18997            -0.010204             0.160097        0.0   0.904633   \n","18998             0.001099            -0.062985        0.0   0.889572   \n","18999            -0.006460            -0.066961        0.0   0.882910   \n","\n","       avg_outgoing_order_first_30  num_incoming_first_30  \\\n","0                        16.555556                   21.0   \n","1                        15.000000                   22.0   \n","2                        13.142857                   23.0   \n","3                        16.555556                   21.0   \n","4                        15.000000                   22.0   \n","...                            ...                    ...   \n","18995                    13.142857                   23.0   \n","18996                    13.142857                   23.0   \n","18997                    14.750000                   22.0   \n","18998                    13.142857                   23.0   \n","18999                    13.142857                   23.0   \n","\n","       incoming_packet_ratio  \n","0                   0.914849  \n","1                   0.845560  \n","2                   0.913108  \n","3                   0.915629  \n","4                   0.918208  \n","...                      ...  \n","18995               0.934386  \n","18996               0.944556  \n","18997               0.941821  \n","18998               0.930486  \n","18999               0.923728  \n","\n","[19000 rows x 13 columns]\n","0         0\n","1         0\n","2         0\n","3         0\n","4         0\n","         ..\n","18995    94\n","18996    94\n","18997    94\n","18998    94\n","18999    94\n","Name: website_label, Length: 19000, dtype: int64\n"]}]},{"cell_type":"markdown","source":["### 2. SVM"],"metadata":{"id":"h1eQls079BzZ"}},{"cell_type":"markdown","source":["Ignore ConvergenceWarning"],"metadata":{"id":"T8DrseX8Xo-X"}},{"cell_type":"code","source":["import warnings\n","from sklearn.exceptions import ConvergenceWarning\n","warnings.filterwarnings(action='ignore', category=ConvergenceWarning)"],"metadata":{"id":"wei-xc2XXotK","executionInfo":{"status":"ok","timestamp":1763946046239,"user_tz":-540,"elapsed":5027,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["Split the dataset into training and testing sets"],"metadata":{"id":"ag7TNuQIXttl"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"],"metadata":{"id":"BzTWApNiXtNG","executionInfo":{"status":"ok","timestamp":1763946047133,"user_tz":-540,"elapsed":876,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Map all points to have mean=0 and std=1"],"metadata":{"id":"5tGt6RZyX77d"}},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","X_train_scale = scaler.fit_transform(X_train.values)\n","X_test_scale = scaler.transform(X_test.values)\n","X_scale = scaler.fit_transform(X.values)"],"metadata":{"id":"yVgWAxkuX8T7","executionInfo":{"status":"ok","timestamp":1763946047200,"user_tz":-540,"elapsed":50,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Train and test SVM using RBF kernel"],"metadata":{"id":"QQ-XuWD_jrtz"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","clf_rbf = SVC(kernel='rbf', C=100, gamma=0.1, class_weight='balanced')\n","clf_rbf.fit(X_train_scale, y_train)\n","y_pred_rbf = clf_rbf.predict(X_test_scale)\n","\n","print(\"============ Before Hyperparameter tuning ============\")\n","print(\"SVM Accuracy: {}\".format(accuracy_score(y_test, y_pred_rbf)))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rbf))"],"metadata":{"id":"k9q3WURZjgco","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763946065546,"user_tz":-540,"elapsed":18328,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"6a39fa37-a145-4624-f0a5-45f51c22d627"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["============ Before Hyperparameter tuning ============\n","SVM Accuracy: 0.816\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.78      0.75      0.76        51\n","           1       0.89      0.72      0.80        47\n","           2       0.84      0.96      0.89        48\n","           3       0.88      1.00      0.94        37\n","           4       0.80      0.80      0.80        44\n","           5       0.83      0.88      0.85        56\n","           6       0.87      0.90      0.88        58\n","           7       0.76      0.79      0.78        53\n","           8       0.67      0.69      0.68        49\n","           9       0.66      0.66      0.66        38\n","          10       0.81      0.85      0.83        55\n","          11       0.69      0.80      0.74        45\n","          12       0.90      0.95      0.92        55\n","          13       0.63      0.60      0.61        45\n","          14       0.69      0.71      0.70        52\n","          15       0.95      0.81      0.88        48\n","          16       0.74      0.80      0.77        50\n","          17       0.73      0.78      0.75        59\n","          18       0.98      0.98      0.98        47\n","          19       0.57      0.65      0.61        46\n","          20       0.96      0.96      0.96        55\n","          21       0.78      0.81      0.80        48\n","          22       0.69      0.80      0.74        46\n","          23       0.89      0.87      0.88        46\n","          24       0.52      0.52      0.52        58\n","          25       0.74      0.71      0.72        48\n","          26       0.80      0.89      0.84        45\n","          27       0.93      0.82      0.87        50\n","          28       0.97      0.88      0.93        42\n","          29       0.84      0.81      0.82        57\n","          30       0.77      0.84      0.80        55\n","          31       0.78      0.88      0.83        43\n","          32       0.80      0.77      0.78        52\n","          33       0.77      0.91      0.83        47\n","          34       0.65      0.62      0.63        53\n","          35       0.93      0.84      0.88        45\n","          36       0.91      0.93      0.92        45\n","          37       0.73      0.66      0.69        62\n","          38       0.78      0.78      0.78        58\n","          39       0.75      0.72      0.73        53\n","          40       0.84      0.85      0.84        60\n","          41       0.90      0.83      0.86        53\n","          42       0.61      0.67      0.64        49\n","          43       0.90      0.96      0.93        46\n","          44       0.96      0.98      0.97        53\n","          45       0.70      0.58      0.63        45\n","          46       0.83      0.80      0.81        44\n","          47       0.80      0.76      0.78        51\n","          48       0.93      0.93      0.93        46\n","          49       0.91      0.81      0.86        52\n","          50       0.85      0.92      0.88        37\n","          51       0.86      0.73      0.79        49\n","          52       0.86      0.86      0.86        57\n","          53       0.95      0.90      0.92        39\n","          54       0.89      0.81      0.85        59\n","          55       0.61      0.64      0.62        44\n","          56       0.93      0.95      0.94        57\n","          57       0.90      0.94      0.92        47\n","          58       0.98      0.90      0.93        48\n","          59       0.91      0.98      0.94        51\n","          60       0.78      0.78      0.78        50\n","          61       0.80      0.78      0.79        68\n","          62       0.93      0.74      0.82        68\n","          63       0.78      0.86      0.82        42\n","          64       0.79      0.86      0.82        49\n","          65       0.75      0.81      0.78        48\n","          66       0.82      0.86      0.84        59\n","          67       0.90      0.90      0.90        50\n","          68       0.91      0.79      0.84        61\n","          69       0.84      0.85      0.85        55\n","          70       0.93      0.84      0.88        45\n","          71       0.70      0.91      0.79        44\n","          72       0.74      0.67      0.71        52\n","          73       0.86      0.93      0.89        54\n","          74       0.81      0.78      0.79        59\n","          75       1.00      1.00      1.00        44\n","          76       0.92      0.92      0.92        51\n","          77       0.70      0.63      0.67        41\n","          78       0.77      0.84      0.80        51\n","          79       0.69      0.52      0.59        42\n","          80       0.96      0.92      0.94        52\n","          81       0.88      0.79      0.83        56\n","          82       0.89      0.74      0.81        57\n","          83       0.69      0.90      0.78        40\n","          84       0.92      0.89      0.90        53\n","          85       0.90      0.96      0.93        56\n","          86       0.82      1.00      0.90        42\n","          87       0.84      0.82      0.83        45\n","          88       0.78      0.82      0.80        44\n","          89       0.62      0.58      0.60        45\n","          90       0.86      0.79      0.82        47\n","          91       0.86      0.86      0.86        56\n","          92       0.76      0.70      0.73        44\n","          93       0.94      0.94      0.94        47\n","          94       0.77      0.73      0.75        55\n","\n","    accuracy                           0.82      4750\n","   macro avg       0.82      0.82      0.82      4750\n","weighted avg       0.82      0.82      0.82      4750\n","\n"]}]},{"cell_type":"markdown","source":["### 3. Hyperparameter tuning by using Grid Search"],"metadata":{"id":"0cDS665_y8tg"}},{"cell_type":"code","source":["from sklearn.experimental import enable_halving_search_cv\n","from sklearn.model_selection import HalvingGridSearchCV\n","\n","param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n","              'gamma': [0.001, 0.01, 0.1, 1],\n","              'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n","\n","search = HalvingGridSearchCV(SVC(class_weight='balanced'), param_grid, cv=5, factor=3, n_jobs=-1)\n","search.fit(X_train_scale, y_train)\n","\n","print(search.best_params_)\n","print(search.best_estimator_)"],"metadata":{"id":"uM3lBZ0ohJOh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763946960100,"user_tz":-540,"elapsed":894549,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"db7b1e56-a22f-4e98-bb7b-346dc2856550"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}\n","SVC(C=100, class_weight='balanced', gamma=0.1)\n"]}]},{"cell_type":"code","source":["y_pred_tuned = search.predict(X_test_scale)\n","\n","print(\"============ After Hyperparameter tuning ============\")\n","print(\"SVM Accuracy: {}\".format(accuracy_score(y_test, y_pred_tuned)))\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tuned))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8aT52fT4X0St","executionInfo":{"status":"ok","timestamp":1763946970995,"user_tz":-540,"elapsed":10903,"user":{"displayName":"‎김민(엘텍공과대학 소프트웨어학부)","userId":"16304554348809022300"}},"outputId":"c9034b44-af0f-4368-b4ec-8b026921b527"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["============ After Hyperparameter tuning ============\n","SVM Accuracy: 0.816\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","           0       0.78      0.75      0.76        51\n","           1       0.89      0.72      0.80        47\n","           2       0.84      0.96      0.89        48\n","           3       0.88      1.00      0.94        37\n","           4       0.80      0.80      0.80        44\n","           5       0.83      0.88      0.85        56\n","           6       0.87      0.90      0.88        58\n","           7       0.76      0.79      0.78        53\n","           8       0.67      0.69      0.68        49\n","           9       0.66      0.66      0.66        38\n","          10       0.81      0.85      0.83        55\n","          11       0.69      0.80      0.74        45\n","          12       0.90      0.95      0.92        55\n","          13       0.63      0.60      0.61        45\n","          14       0.69      0.71      0.70        52\n","          15       0.95      0.81      0.88        48\n","          16       0.74      0.80      0.77        50\n","          17       0.73      0.78      0.75        59\n","          18       0.98      0.98      0.98        47\n","          19       0.57      0.65      0.61        46\n","          20       0.96      0.96      0.96        55\n","          21       0.78      0.81      0.80        48\n","          22       0.69      0.80      0.74        46\n","          23       0.89      0.87      0.88        46\n","          24       0.52      0.52      0.52        58\n","          25       0.74      0.71      0.72        48\n","          26       0.80      0.89      0.84        45\n","          27       0.93      0.82      0.87        50\n","          28       0.97      0.88      0.93        42\n","          29       0.84      0.81      0.82        57\n","          30       0.77      0.84      0.80        55\n","          31       0.78      0.88      0.83        43\n","          32       0.80      0.77      0.78        52\n","          33       0.77      0.91      0.83        47\n","          34       0.65      0.62      0.63        53\n","          35       0.93      0.84      0.88        45\n","          36       0.91      0.93      0.92        45\n","          37       0.73      0.66      0.69        62\n","          38       0.78      0.78      0.78        58\n","          39       0.75      0.72      0.73        53\n","          40       0.84      0.85      0.84        60\n","          41       0.90      0.83      0.86        53\n","          42       0.61      0.67      0.64        49\n","          43       0.90      0.96      0.93        46\n","          44       0.96      0.98      0.97        53\n","          45       0.70      0.58      0.63        45\n","          46       0.83      0.80      0.81        44\n","          47       0.80      0.76      0.78        51\n","          48       0.93      0.93      0.93        46\n","          49       0.91      0.81      0.86        52\n","          50       0.85      0.92      0.88        37\n","          51       0.86      0.73      0.79        49\n","          52       0.86      0.86      0.86        57\n","          53       0.95      0.90      0.92        39\n","          54       0.89      0.81      0.85        59\n","          55       0.61      0.64      0.62        44\n","          56       0.93      0.95      0.94        57\n","          57       0.90      0.94      0.92        47\n","          58       0.98      0.90      0.93        48\n","          59       0.91      0.98      0.94        51\n","          60       0.78      0.78      0.78        50\n","          61       0.80      0.78      0.79        68\n","          62       0.93      0.74      0.82        68\n","          63       0.78      0.86      0.82        42\n","          64       0.79      0.86      0.82        49\n","          65       0.75      0.81      0.78        48\n","          66       0.82      0.86      0.84        59\n","          67       0.90      0.90      0.90        50\n","          68       0.91      0.79      0.84        61\n","          69       0.84      0.85      0.85        55\n","          70       0.93      0.84      0.88        45\n","          71       0.70      0.91      0.79        44\n","          72       0.74      0.67      0.71        52\n","          73       0.86      0.93      0.89        54\n","          74       0.81      0.78      0.79        59\n","          75       1.00      1.00      1.00        44\n","          76       0.92      0.92      0.92        51\n","          77       0.70      0.63      0.67        41\n","          78       0.77      0.84      0.80        51\n","          79       0.69      0.52      0.59        42\n","          80       0.96      0.92      0.94        52\n","          81       0.88      0.79      0.83        56\n","          82       0.89      0.74      0.81        57\n","          83       0.69      0.90      0.78        40\n","          84       0.92      0.89      0.90        53\n","          85       0.90      0.96      0.93        56\n","          86       0.82      1.00      0.90        42\n","          87       0.84      0.82      0.83        45\n","          88       0.78      0.82      0.80        44\n","          89       0.62      0.58      0.60        45\n","          90       0.86      0.79      0.82        47\n","          91       0.86      0.86      0.86        56\n","          92       0.76      0.70      0.73        44\n","          93       0.94      0.94      0.94        47\n","          94       0.77      0.73      0.75        55\n","\n","    accuracy                           0.82      4750\n","   macro avg       0.82      0.82      0.82      4750\n","weighted avg       0.82      0.82      0.82      4750\n","\n"]}]}]}